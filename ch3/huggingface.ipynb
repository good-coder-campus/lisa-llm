{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-03T11:16:30.917637Z",
     "start_time": "2025-02-03T11:16:19.276406Z"
    }
   },
   "source": [
    "from idlelib.browser import transform_children\n",
    "\n",
    "from sympy.stats.rv import probability\n",
    "from transformers import AutoModel\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44c8813ceac64e40930b5d7e531d1bb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "042f745185504f38a148c8cf821d9d6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:16:47.775784Z",
     "start_time": "2025-02-03T11:16:46.953759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_id = 'SamLowe/roberta-base-go-emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ],
   "id": "fc4a9dba2ccc5d5f",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SamLowe/roberta-base-go-emotions is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/SamLowe/roberta-base-go-emotions/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:402\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 402\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m hf_hub_download(\n\u001B[1;32m    403\u001B[0m         path_or_repo_id,\n\u001B[1;32m    404\u001B[0m         filename,\n\u001B[1;32m    405\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(subfolder) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m subfolder,\n\u001B[1;32m    406\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[1;32m    407\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    408\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    409\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m    410\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    411\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    412\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m    413\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    414\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    415\u001B[0m     )\n\u001B[1;32m    416\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:101\u001B[0m, in \u001B[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    100\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1240\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_cache_dir(\n\u001B[1;32m   1241\u001B[0m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[1;32m   1242\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m   1243\u001B[0m         \u001B[38;5;66;03m# File info\u001B[39;00m\n\u001B[1;32m   1244\u001B[0m         repo_id\u001B[38;5;241m=\u001B[39mrepo_id,\n\u001B[1;32m   1245\u001B[0m         filename\u001B[38;5;241m=\u001B[39mfilename,\n\u001B[1;32m   1246\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[1;32m   1247\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m   1248\u001B[0m         \u001B[38;5;66;03m# HTTP info\u001B[39;00m\n\u001B[1;32m   1249\u001B[0m         endpoint\u001B[38;5;241m=\u001B[39mendpoint,\n\u001B[1;32m   1250\u001B[0m         etag_timeout\u001B[38;5;241m=\u001B[39metag_timeout,\n\u001B[1;32m   1251\u001B[0m         headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m   1252\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m   1253\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m   1254\u001B[0m         \u001B[38;5;66;03m# Additional options\u001B[39;00m\n\u001B[1;32m   1255\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m   1256\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m   1257\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1347\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[0m\n\u001B[1;32m   1346\u001B[0m     \u001B[38;5;66;03m# Otherwise, raise appropriate error\u001B[39;00m\n\u001B[0;32m-> 1347\u001B[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001B[1;32m   1349\u001B[0m \u001B[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1854\u001B[0m, in \u001B[0;36m_raise_on_head_call_error\u001B[0;34m(head_call_error, force_download, local_files_only)\u001B[0m\n\u001B[1;32m   1852\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, RepositoryNotFoundError) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, GatedRepoError):\n\u001B[1;32m   1853\u001B[0m     \u001B[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001B[39;00m\n\u001B[0;32m-> 1854\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m head_call_error\n\u001B[1;32m   1855\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1856\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1751\u001B[0m, in \u001B[0;36m_get_metadata_or_catch_error\u001B[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001B[0m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m get_hf_file_metadata(\n\u001B[1;32m   1752\u001B[0m         url\u001B[38;5;241m=\u001B[39murl, proxies\u001B[38;5;241m=\u001B[39mproxies, timeout\u001B[38;5;241m=\u001B[39metag_timeout, headers\u001B[38;5;241m=\u001B[39mheaders, token\u001B[38;5;241m=\u001B[39mtoken\n\u001B[1;32m   1753\u001B[0m     )\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1673\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[0m\n\u001B[1;32m   1672\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[0;32m-> 1673\u001B[0m r \u001B[38;5;241m=\u001B[39m _request_wrapper(\n\u001B[1;32m   1674\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHEAD\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1675\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m   1676\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m   1677\u001B[0m     allow_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1678\u001B[0m     follow_relative_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   1679\u001B[0m     proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m   1680\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m   1681\u001B[0m )\n\u001B[1;32m   1682\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:376\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[0;32m--> 376\u001B[0m     response \u001B[38;5;241m=\u001B[39m _request_wrapper(\n\u001B[1;32m    377\u001B[0m         method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[1;32m    378\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m    379\u001B[0m         follow_relative_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    380\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    381\u001B[0m     )\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[1;32m    384\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:400\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    399\u001B[0m response \u001B[38;5;241m=\u001B[39m get_session()\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m--> 400\u001B[0m hf_raise_for_status(response)\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:352\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    344\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    345\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Client Error.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    346\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    350\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make sure you are authenticated.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    351\u001B[0m     )\n\u001B[0;32m--> 352\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RepositoryNotFoundError(message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m400\u001B[39m:\n",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m: 401 Client Error. (Request ID: Root=1-67a0a59f-306c8ef205d38f0a187b5daa;844d236a-3b90-4c3a-8c98-8ec2fe58079a)\n\nRepository Not Found for url: https://huggingface.co/SamLowe/roberta-base-go-emotions/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForSequenceClassification\n\u001B[1;32m      3\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSamLowe/roberta-base-go-emotions\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 4\u001B[0m classification_model \u001B[38;5;241m=\u001B[39m AutoModelForSequenceClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:485\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m commit_hash \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, PretrainedConfig):\n\u001B[1;32m    484\u001B[0m         \u001B[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001B[39;00m\n\u001B[0;32m--> 485\u001B[0m         resolved_config_file \u001B[38;5;241m=\u001B[39m cached_file(\n\u001B[1;32m    486\u001B[0m             pretrained_model_name_or_path,\n\u001B[1;32m    487\u001B[0m             CONFIG_NAME,\n\u001B[1;32m    488\u001B[0m             _raise_exceptions_for_gated_repo\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    489\u001B[0m             _raise_exceptions_for_missing_entries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    490\u001B[0m             _raise_exceptions_for_connection_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    491\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs,\n\u001B[1;32m    492\u001B[0m         )\n\u001B[1;32m    493\u001B[0m         commit_hash \u001B[38;5;241m=\u001B[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001B[1;32m    494\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:425\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    420\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    421\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure to have access to it at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    422\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    423\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    424\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 425\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    426\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a local folder and is not a valid model identifier \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    427\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlisted on \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf this is a private repository, make sure to pass a token \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    428\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    429\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`token=<your_token>`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    430\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    431\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RevisionNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    432\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    433\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    434\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor this model name. Check the model page at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    435\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for available revisions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    436\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mOSError\u001B[0m: SamLowe/roberta-base-go-emotions is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:17:27.336140Z",
     "start_time": "2025-02-03T11:17:25.009904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "id": "9eb69b59076019bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "962f861e079143dfa7dbc3fcb0c47318"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "942782118002422280a17d7448156eb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/752k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e1031afc7b94315902485acbe956a41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcabc9010dd24ca083769b4f3fbf7fd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:17:43.955117Z",
     "start_time": "2025-02-03T11:17:43.952307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized = tokenizer(\"토크나이저는 텍스트를 토큰 단위로 나눈다\")\n",
    "print(tokenized)"
   ],
   "id": "3d8f5a873a19edc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:00.391149Z",
     "start_time": "2025-02-03T11:20:00.388293Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))",
   "id": "b1a4dfc925eae636",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:35.309767Z",
     "start_time": "2025-02-03T11:20:00.990351Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode(tokenized['input_ids']))",
   "id": "72b4cffefaf06879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:35.313211Z",
     "start_time": "2025-02-03T11:20:35.310742Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))",
   "id": "62e24c7eb76e7dc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저는 텍스트를 토큰 단위로 나눈다\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:35.317161Z",
     "start_time": "2025-02-03T11:20:35.313835Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer(['첫 번째 문장', '두 번째 문장'])",
   "id": "6b16c89b2c6063b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:35.320272Z",
     "start_time": "2025-02-03T11:20:35.318219Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer([['첫 번째 문장', '두 번째 문장']])",
   "id": "a35a81f42f75dfc7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:35.323081Z",
     "start_time": "2025-02-03T11:20:35.320757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_tokenized_result = tokenizer(['첫 번째 문장', '두 번째 문장'])['input_ids']\n",
    "tokenizer.batch_decode(first_tokenized_result)"
   ],
   "id": "a0f4325462419bb9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:35.326008Z",
     "start_time": "2025-02-03T11:20:35.323589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "second_tokenized_result = tokenizer([['첫 번째 문장', '두 번째 문장']])['input_ids']\n",
    "tokenizer.batch_decode(second_tokenized_result)"
   ],
   "id": "2dd6df0b7fb327fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:37.929446Z",
     "start_time": "2025-02-03T11:20:35.326645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "bert_tokenizer([['첫 번째 문장', '두 번째 문장']])"
   ],
   "id": "74593727e383f52f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bebcf46eb03e4c419d95c908100ad910"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ce7324ed8c84adabbf28f7ce213a5f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccecdbf0b28940aa827f1e5a9bb04922"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e39b9757c6842bdb3ea578348402f38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92b80e40cc344427af0ac574fd833d2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:38.158554Z",
     "start_time": "2025-02-03T11:20:37.930301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "roberta_tokenizer([['첫 번째 문장', '두 번째 문장']])"
   ],
   "id": "9e8f3f71c048b17",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:41.921576Z",
     "start_time": "2025-02-03T11:20:38.159589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "en_roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "en_roberta_tokenizer([['first sentence', 'second sentence']])"
   ],
   "id": "27662e0b6ea42c70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c731643eee634a0684c8fe55c658fd6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e46d666beff64f30a536625fbb7e25d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae9f219f22c94ea3b3db91a72e833d95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6db187a2b23429f8ddeb64d24e7c154"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "248781b577c04daf87371948cfa8fdf9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:20:41.926371Z",
     "start_time": "2025-02-03T11:20:41.923690Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer(['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장보다 더 길다.'], padding='longest')",
   "id": "362a48dde6de0831",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 2178, 2062, 831, 647, 2062, 18, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:21:01.954261Z",
     "start_time": "2025-02-03T11:20:41.927090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "klue_mrc_dataset = load_dataset('klue', 'mrc')\n",
    "klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train')"
   ],
   "id": "edcdcf0e8ac92fc1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/22.5k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5afaa841bbe8483ab71c26b8aa581f0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48314c75c4f24e1f95f83e08d14ee640"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe8dc24b5f4148f2bf12f48813074e54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/17554 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dafb6bfed90f40caa6f71d986543a3d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/5841 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fac9e5fa46d54635aa050c0643f304b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T04:21:19.158312Z",
     "start_time": "2025-01-29T04:21:19.153387Z"
    }
   },
   "cell_type": "code",
   "source": "klue_mrc_dataset",
   "id": "6e4d28928f7755ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
       "        num_rows: 17554\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
       "        num_rows: 5841\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:22:25.609929Z",
     "start_time": "2025-02-03T11:22:24.492288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 로컬의 csv 데이터 파일 활용\n",
    "dataset = load_dataset('csv', data_files='data.csv')\n",
    "\n",
    "from datasets import Dataset\n",
    "my_dict = {\"a\": [1, 2, 3]}\n",
    "dataset = Dataset.from_dict(my_dict)\n",
    "\n",
    "# 판다스 데이터프레임 활용\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "dataset = Dataset.from_pandas(df)"
   ],
   "id": "5fbf7efb9cdda31c",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/Users/harim/repo/lisa-llm/ch3/data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# 로컬의 csv 데이터 파일 활용\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m dataset \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m'\u001B[39m, data_files\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[1;32m      7\u001B[0m my_dict \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m]}\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:2594\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[1;32m   2589\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[1;32m   2590\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[1;32m   2591\u001B[0m )\n\u001B[1;32m   2593\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[0;32m-> 2594\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m load_dataset_builder(\n\u001B[1;32m   2595\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[1;32m   2596\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[1;32m   2597\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[1;32m   2598\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[1;32m   2599\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m   2600\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[1;32m   2601\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[1;32m   2602\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[1;32m   2603\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m   2604\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m   2605\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m   2606\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[1;32m   2607\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39mname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   2608\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[1;32m   2609\u001B[0m )\n\u001B[1;32m   2611\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[1;32m   2612\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:2266\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001B[0m\n\u001B[1;32m   2264\u001B[0m     download_config \u001B[38;5;241m=\u001B[39m download_config\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m download_config \u001B[38;5;28;01melse\u001B[39;00m DownloadConfig()\n\u001B[1;32m   2265\u001B[0m     download_config\u001B[38;5;241m.\u001B[39mstorage_options\u001B[38;5;241m.\u001B[39mupdate(storage_options)\n\u001B[0;32m-> 2266\u001B[0m dataset_module \u001B[38;5;241m=\u001B[39m dataset_module_factory(\n\u001B[1;32m   2267\u001B[0m     path,\n\u001B[1;32m   2268\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m   2269\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[1;32m   2270\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[1;32m   2271\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[1;32m   2272\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[1;32m   2273\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m   2274\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[1;32m   2275\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39m_require_default_config_name,\n\u001B[1;32m   2276\u001B[0m     _require_custom_configs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m(config_kwargs),\n\u001B[1;32m   2277\u001B[0m )\n\u001B[1;32m   2278\u001B[0m \u001B[38;5;66;03m# Get dataset builder class from the processing script\u001B[39;00m\n\u001B[1;32m   2279\u001B[0m builder_kwargs \u001B[38;5;241m=\u001B[39m dataset_module\u001B[38;5;241m.\u001B[39mbuilder_kwargs\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1805\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001B[0m\n\u001B[1;32m   1782\u001B[0m \u001B[38;5;66;03m# We have several ways to get a dataset builder:\u001B[39;00m\n\u001B[1;32m   1783\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m   1784\u001B[0m \u001B[38;5;66;03m# - if path is the name of a packaged dataset module\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1796\u001B[0m \n\u001B[1;32m   1797\u001B[0m \u001B[38;5;66;03m# Try packaged\u001B[39;00m\n\u001B[1;32m   1798\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m _PACKAGED_DATASETS_MODULES:\n\u001B[1;32m   1799\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m PackagedDatasetModuleFactory(\n\u001B[1;32m   1800\u001B[0m         path,\n\u001B[1;32m   1801\u001B[0m         data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[1;32m   1802\u001B[0m         data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[1;32m   1803\u001B[0m         download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[1;32m   1804\u001B[0m         download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m-> 1805\u001B[0m     )\u001B[38;5;241m.\u001B[39mget_module()\n\u001B[1;32m   1806\u001B[0m \u001B[38;5;66;03m# Try locally\u001B[39;00m\n\u001B[1;32m   1807\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path\u001B[38;5;241m.\u001B[39mendswith(filename):\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1142\u001B[0m, in \u001B[0;36mPackagedDatasetModuleFactory.get_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1136\u001B[0m base_path \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mexpanduser()\u001B[38;5;241m.\u001B[39mresolve()\u001B[38;5;241m.\u001B[39mas_posix()\n\u001B[1;32m   1137\u001B[0m patterns \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1138\u001B[0m     sanitize_patterns(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files)\n\u001B[1;32m   1139\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1140\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m get_data_patterns(base_path, download_config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_config)\n\u001B[1;32m   1141\u001B[0m )\n\u001B[0;32m-> 1142\u001B[0m data_files \u001B[38;5;241m=\u001B[39m DataFilesDict\u001B[38;5;241m.\u001B[39mfrom_patterns(\n\u001B[1;32m   1143\u001B[0m     patterns,\n\u001B[1;32m   1144\u001B[0m     download_config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_config,\n\u001B[1;32m   1145\u001B[0m     base_path\u001B[38;5;241m=\u001B[39mbase_path,\n\u001B[1;32m   1146\u001B[0m )\n\u001B[1;32m   1147\u001B[0m supports_metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;129;01min\u001B[39;00m _MODULE_SUPPORTS_METADATA\n\u001B[1;32m   1148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m supports_metadata \u001B[38;5;129;01mand\u001B[39;00m patterns \u001B[38;5;241m!=\u001B[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/data_files.py:721\u001B[0m, in \u001B[0;36mDataFilesDict.from_patterns\u001B[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[1;32m    716\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m()\n\u001B[1;32m    717\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, patterns_for_key \u001B[38;5;129;01min\u001B[39;00m patterns\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    718\u001B[0m     out[key] \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    719\u001B[0m         patterns_for_key\n\u001B[1;32m    720\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(patterns_for_key, DataFilesList)\n\u001B[0;32m--> 721\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m DataFilesList\u001B[38;5;241m.\u001B[39mfrom_patterns(\n\u001B[1;32m    722\u001B[0m             patterns_for_key,\n\u001B[1;32m    723\u001B[0m             base_path\u001B[38;5;241m=\u001B[39mbase_path,\n\u001B[1;32m    724\u001B[0m             allowed_extensions\u001B[38;5;241m=\u001B[39mallowed_extensions,\n\u001B[1;32m    725\u001B[0m             download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[1;32m    726\u001B[0m         )\n\u001B[1;32m    727\u001B[0m     )\n\u001B[1;32m    728\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/data_files.py:624\u001B[0m, in \u001B[0;36mDataFilesList.from_patterns\u001B[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[1;32m    621\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pattern \u001B[38;5;129;01min\u001B[39;00m patterns:\n\u001B[1;32m    622\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    623\u001B[0m         data_files\u001B[38;5;241m.\u001B[39mextend(\n\u001B[0;32m--> 624\u001B[0m             resolve_pattern(\n\u001B[1;32m    625\u001B[0m                 pattern,\n\u001B[1;32m    626\u001B[0m                 base_path\u001B[38;5;241m=\u001B[39mbase_path,\n\u001B[1;32m    627\u001B[0m                 allowed_extensions\u001B[38;5;241m=\u001B[39mallowed_extensions,\n\u001B[1;32m    628\u001B[0m                 download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[1;32m    629\u001B[0m             )\n\u001B[1;32m    630\u001B[0m         )\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_magic(pattern):\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/data_files.py:411\u001B[0m, in \u001B[0;36mresolve_pattern\u001B[0;34m(pattern, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[1;32m    409\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m allowed_extensions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    410\u001B[0m         error_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m with any supported extension \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(allowed_extensions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 411\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(error_msg)\n\u001B[1;32m    412\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: Unable to find '/Users/harim/repo/lisa-llm/ch3/data.csv'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:22:50.566581Z",
     "start_time": "2025-02-03T11:22:34.297958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "klue_tc_train = load_dataset('klue', 'ynat', split='train')\n",
    "klue_tc_eval = load_dataset('klue', 'ynat', split='validation')\n",
    "klue_tc_train"
   ],
   "id": "4a1bd451479a3a6f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.17M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee11bc6ed6f342d2a178d38a01504621"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/847k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "969faf55e7a6491db560873fc59a1890"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/45678 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cdd3e5c09d54b9b93e83b491621de47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/9107 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fefdbe3c12e0460c83c9724f70c1d4e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['guid', 'title', 'label', 'url', 'date'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:22:58.271485Z",
     "start_time": "2025-02-03T11:22:58.267096Z"
    }
   },
   "cell_type": "code",
   "source": "klue_tc_train[0]",
   "id": "f78ddbb7cad1b30f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:22:59.509582Z",
     "start_time": "2025-02-03T11:22:59.505240Z"
    }
   },
   "cell_type": "code",
   "source": "klue_tc_train.features['label'].names",
   "id": "194f7fb2cd011477",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:23:00.012309Z",
     "start_time": "2025-02-03T11:23:00.008125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "klue_tc_train = klue_tc_train.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_eval = klue_tc_eval.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_train\n"
   ],
   "id": "2d7171ee8a85f0af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'label'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:24:59.410848Z",
     "start_time": "2025-02-03T11:24:59.363232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "klue_tc_label = klue_tc_train.features['label']\n",
    "\n",
    "def make_str_label(batch):\n",
    "    batch['label_str'] = klue_tc_label.int2str(batch['label'])\n",
    "    return batch\n",
    "\n",
    "klue_tc_train = klue_tc_train.map(make_str_label, batched=True, batch_size=1000)\n",
    "\n",
    "klue_tc_train[0]"
   ],
   "id": "d14fd3072f30860e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/45678 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a745f7ddcb28491c831c205cab1a167a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'labael_str': '생활문화',\n",
       " 'label_str': '생활문화'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:25:23.505974Z",
     "start_time": "2025-02-03T11:25:23.484485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = klue_tc_train.train_test_split(test_size=10000, shuffle=True, seed=42)['test']\n",
    "dataset = klue_tc_eval.train_test_split(test_size=1000, shuffle=True, seed=42)\n",
    "test_dataset = dataset['test']\n",
    "valid_dataset = dataset['train'].train_test_split(test_size=1000, shuffle=True, seed=42)['test']\n"
   ],
   "id": "d07b6adda9630a27",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:25:36.529253Z",
     "start_time": "2025-02-03T11:25:34.700359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['title'], padding='max_length', truncation=True)\n",
    "\n",
    "model_id = \"klue/roberta-base\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "id": "d6bd23755d448406",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:27:17.108216Z",
     "start_time": "2025-02-03T11:27:16.077308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ],
   "id": "43cc2927a0b8930a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a72f8bb03a74311a8ebc2e108f583d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:27:37.376579Z",
     "start_time": "2025-02-03T11:27:37.371855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "\toutput_dir=\"./results\", # 결과 저장할 폴더\n",
    "\tnum_train_epochs=1, # 학습 에포크 수\n",
    "\tper_device_train_batch_size=8, # 배치 크기\n",
    "\tper_device_eval_batch_size=8, # 배치 크기\n",
    "\tevaluation_strategy=\"epoch\", # 한 에포크 학습 끝날 때마다 검증 데이터셋에 대한 평가 수행하도록 설정\n",
    "\tlearning_rate=5e-5,\n",
    "\tpush_to_hub=False\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred): # 평가 지표(정확도)\n",
    "\tlogits, labels = eval_pred\n",
    "\tpredictions = np.argmax(logits, axis=-1)\n",
    "\treturn {\"accuracy\": (predictions == labels).mean()}"
   ],
   "id": "923fd065c7cfe467",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T11:27:41.847984Z",
     "start_time": "2025-02-03T11:27:41.808034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\targs=training_args,\n",
    "\ttrain_dataset=train_dataset,\n",
    "\teval_dataset=valid_dataset,\n",
    "\ttokenizer=tokenizer,\n",
    "\tcompute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate(test_dataset) # 정확도 0.84"
   ],
   "id": "b60f2c37a7adc4a0",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFRobertaForSequenceClassification' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m      2\u001B[0m \tmodel\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m      3\u001B[0m \targs\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m      4\u001B[0m \ttrain_dataset\u001B[38;5;241m=\u001B[39mtrain_dataset,\n\u001B[1;32m      5\u001B[0m \teval_dataset\u001B[38;5;241m=\u001B[39mvalid_dataset,\n\u001B[1;32m      6\u001B[0m \ttokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[1;32m      7\u001B[0m \tcompute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics,\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m     10\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     12\u001B[0m trainer\u001B[38;5;241m.\u001B[39mevaluate(test_dataset)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:535\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001B[39;00m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    532\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplace_model_on_device\n\u001B[1;32m    533\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_method\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m QuantizationMethod\u001B[38;5;241m.\u001B[39mBITS_AND_BYTES\n\u001B[1;32m    534\u001B[0m ):\n\u001B[0;32m--> 535\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_move_model_to_device(model, args\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    537\u001B[0m \u001B[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001B[39;00m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_model_parallel:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:782\u001B[0m, in \u001B[0;36mTrainer._move_model_to_device\u001B[0;34m(self, model, device)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_move_model_to_device\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, device):\n\u001B[0;32m--> 782\u001B[0m     model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001B[39;00m\n\u001B[1;32m    784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mparallel_mode \u001B[38;5;241m==\u001B[39m ParallelMode\u001B[38;5;241m.\u001B[39mTPU \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtie_weights\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'TFRobertaForSequenceClassification' object has no attribute 'to'"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.4.3 트레이너 API 를 사용하지 않고 학습하기\n",
    "위 예시와 비슷하게 모델과 토크나이저를 불러오고, 토큰화에 사용할 tokenize_function 정의\n",
    "Trainer가 내부적으로 수행하던 GPU로의 모델 이동(model.to(device))을 직접 수행해야 함"
   ],
   "id": "b870dc33e2cb1eba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T04:22:55.200367Z",
     "start_time": "2025-01-29T04:22:55.185949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, TFAutoModelForSequenceClassification\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"klue/roberta-base\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.to(device)"
   ],
   "id": "e84a9db0fb0a0f94",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mklue/roberta-base\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 12\u001B[0m model \u001B[38;5;241m=\u001B[39m TFAutoModelForSequenceClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id, num_labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241m.\u001B[39mfeatures[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mnames))\n\u001B[1;32m     13\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id)\n\u001B[1;32m     14\u001B[0m model\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T04:22:55.796325Z",
     "start_time": "2025-01-29T04:22:55.781007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_dataloader(dataset, batch_size, shuffle=True):\n",
    "    dataset = dataset.map(tokenize_function, batched=True).with_format(\"torch\")\n",
    "    # 데이터셋에 토큰화 수행\n",
    "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "    dataset = dataset.remove_columns(columne_names=['title'])\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# 데이터로더 만들기\n",
    "train_dataloader = make_dataloader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_dataloader = make_dataloader(valid_dataset, batch_size=8, shuffle=False)\n",
    "test_dataloader = make_dataloader(test_dataset, batch_size=8, shuffle=False)"
   ],
   "id": "ec3899f2f1540acf",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39mshuffle)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# 데이터로더 만들기\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m make_dataloader(\u001B[43mtrain_dataset\u001B[49m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     10\u001B[0m valid_dataloader \u001B[38;5;241m=\u001B[39m make_dataloader(valid_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     11\u001B[0m test_dataloader \u001B[38;5;241m=\u001B[39m make_dataloader(test_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device) # 모델에 입력할 토큰 아이디\n",
    "        attention_mask = batch['attention_mask'].to(device) # 모델에 입력할 어텐션 마스크\n",
    "        labels = batch['labels'].to(device) # 모델에 입력할 레이블\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels) # 모델 계산\n",
    "        loss = outputs.loss # 손실\n",
    "        loss.backward() # 역전파\n",
    "        optimizer.step() # 모델 업데이트\n",
    "        total_loss += loss.item()\n",
    "\tavg_loss = total_loss / len(data_loader)\n",
    "\treturn avg_loss"
   ],
   "id": "59401a48704aa1e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T04:05:16.249529Z",
     "start_time": "2025-01-29T04:05:16.245323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\tavg_loss = total_loss / len(data_loader)\n",
    "\taccuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
    "\treturn avg_loss, accuracy"
   ],
   "id": "9d6feb621200fb5f",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2870116992.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[2], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    def evaluate(model, data_loader)\u001B[0m\n\u001B[0m                                    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m expected ':'\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T04:12:52.175091Z",
     "start_time": "2025-01-29T04:12:52.163274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 1\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_dataloader)\n",
    "    print(f\"Validation loss: {valid_loss}\")\n",
    "    print(f\"Validation accuracy: {valid_accuracy}\")"
   ],
   "id": "d5b7e8633aeb233b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdamW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m----> 2\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m \u001B[43mAdamW\u001B[49m(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5e-5\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 학습 루프\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'AdamW' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.4.4 학습한 모델 업로드하기\n",
    "huggingface_hub 라이브러리로 허깅페이스에 프로그래밍 방식으로 접근 가능\n",
    "업로드 방식\n",
    "- Trainer를 사용한 경우, trainer 인스턴스에서 `push_to_hub()` 사용\n",
    "- 직접 학습한 경우, 모델과 토크나이저를 각각 `push_to_hub()` 로 업로드"
   ],
   "id": "e457e60b01899806"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"token\")\n",
    "repo_id = f\"lhr213213/roberta-base-klue-ynat-classification\"\n",
    "\n",
    "# Trainer 사용한 경우\n",
    "trainer.push_to_hub(repo_id)\n",
    "\n",
    "# 직접 학습한 경우\n",
    "model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)"
   ],
   "id": "f2ccd126e5d97f40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.5 모델 추론\n",
    "- 파이프라인 활용하는 방법\n",
    "- 직접 모델과 토크나이저 불러와 활용하는 방법\n",
    "\n",
    "### 3.5.1 파이프라인 활용한 추론\n",
    "- 작업 종류, 모델, 설정을 입력으로 받아 토크나이저와 모델을 결합해 데이터 전후처리와 모델 추론을 수행함"
   ],
   "id": "28a95f604a163f50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T04:25:43.253808Z",
     "start_time": "2025-01-29T04:25:42.805156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"lhr213213/roberta-base-klue-ynat-classification\"\n",
    "\n",
    "model_pipeline = pipeline(\"text-classification\", model=model_id)\n",
    "\n",
    "model_pipeline(dataset[\"title\"][:5])"
   ],
   "id": "6746a513159d9ba6",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "lhr213213/roberta-base-klue-ynat-classification is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 406\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/requests/models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/lhr213213/roberta-base-klue-ynat-classification/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 403\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    406\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:860\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[1;32m    859\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 860\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[1;32m    862\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[1;32m    864\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    866\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[1;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    873\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[1;32m    875\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:967\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[0m\n\u001B[1;32m    966\u001B[0m     \u001B[38;5;66;03m# Otherwise, raise appropriate error\u001B[39;00m\n\u001B[0;32m--> 967\u001B[0m     \u001B[43m_raise_on_head_call_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead_call_error\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    969\u001B[0m \u001B[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001B[39;00m\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1482\u001B[0m, in \u001B[0;36m_raise_on_head_call_error\u001B[0;34m(head_call_error, force_download, local_files_only)\u001B[0m\n\u001B[1;32m   1480\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, RepositoryNotFoundError) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, GatedRepoError):\n\u001B[1;32m   1481\u001B[0m     \u001B[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001B[39;00m\n\u001B[0;32m-> 1482\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m head_call_error\n\u001B[1;32m   1483\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1484\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1374\u001B[0m, in \u001B[0;36m_get_metadata_or_catch_error\u001B[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001B[0m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1374\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m \u001B[43mget_hf_file_metadata\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1375\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\n\u001B[1;32m   1376\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1377\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1294\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[0m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[0;32m-> 1294\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHEAD\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1296\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1297\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1298\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1299\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1300\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1301\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1302\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1303\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:278\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[0;32m--> 278\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:302\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    301\u001B[0m response \u001B[38;5;241m=\u001B[39m get_session()\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m--> 302\u001B[0m \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:454\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    446\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    447\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Client Error.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    448\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    452\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make sure you are authenticated.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    453\u001B[0m     )\n\u001B[0;32m--> 454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _format(RepositoryNotFoundError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m400\u001B[39m:\n",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m: 401 Client Error. (Request ID: Root=1-6799adc7-2be7289327e01dbf78bfe192;a54ca011-2348-4de0-96c4-c7a01fb19428)\n\nRepository Not Found for url: https://huggingface.co/lhr213213/roberta-base-klue-ynat-classification/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pipeline\n\u001B[1;32m      3\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlhr213213/roberta-base-klue-ynat-classification\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 5\u001B[0m model_pipeline \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-classification\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m model_pipeline(dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m\"\u001B[39m][:\u001B[38;5;241m5\u001B[39m])\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:812\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[1;32m    808\u001B[0m     pretrained_model_name_or_path \u001B[38;5;241m=\u001B[39m model\n\u001B[1;32m    810\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, PretrainedConfig) \u001B[38;5;129;01mand\u001B[39;00m pretrained_model_name_or_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    811\u001B[0m     \u001B[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001B[39;00m\n\u001B[0;32m--> 812\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    813\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    814\u001B[0m \u001B[43m        \u001B[49m\u001B[43mCONFIG_NAME\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    815\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_raise_exceptions_for_gated_repo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    816\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_raise_exceptions_for_missing_entries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    817\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_raise_exceptions_for_connection_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    818\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcache_dir\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    819\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    820\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    821\u001B[0m     hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/repo/lisa-llm/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:426\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    421\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    422\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure to have access to it at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    423\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    424\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 426\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    427\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a local folder and is not a valid model identifier \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    428\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlisted on \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf this is a private repository, make sure to pass a token \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    429\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    430\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`token=<your_token>`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    431\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RevisionNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    434\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    435\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor this model name. Check the model page at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    436\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for available revisions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    437\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mOSError\u001B[0m: lhr213213/roberta-base-klue-ynat-classification is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import TFAutoModelForAudioClassification, AutoTokenizer\n",
    "\n",
    "class CustomPipeline:\n",
    "    def __init__(self, model_id):\n",
    "        self.model = TFAutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, texts):\n",
    "        tokenized = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        probabilities = softmax(logits, dim=-1)\n",
    "        scores, label = torch.max(probabilities, dim=-1)\n",
    "        labels_str = [self.model.config.id2label[label_idx] for label_idx in labels.tolist()]\n",
    "\n",
    "        return [{\"label\": label, \"score\": score.item()} for label, score in zip(labels_str, scores)]\n",
    "\n",
    "custom_pipeline = CustomPipeline(model_id)\n",
    "custom_pipeline(dataset['title'][:5])"
   ],
   "id": "f32004359f689ae9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
